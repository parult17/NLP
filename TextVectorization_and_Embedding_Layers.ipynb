{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, pathlib, shutil, random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Dense\n",
        "from tensorflow.keras.utils import text_dataset_from_directory\n",
        "\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "qnUhfGmx9cup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TextVectorization"
      ],
      "metadata": {
        "id": "KtDuXYqa7cbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dummy dataset and a test sentence\n"
      ],
      "metadata": {
        "id": "8QdHTVk5K_wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\""
      ],
      "metadata": {
        "id": "njuS3JqcS-RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TextVectorization layer and adapt to dummy dataset"
      ],
      "metadata": {
        "id": "M9sp6faVpYYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demonstrate the working of a TextVectorization layer."
      ],
      "metadata": {
        "id": "pB0aHkyhr3D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating a TextVectorization layer/object with output mode as integer\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",              # int is default. There are different kinds of modes available\n",
        "    max_tokens=15,                  # Vocabulary size\n",
        "    output_sequence_length=10,      # Maximum length of output sequence\n",
        "    # We can use custom functions also for standardizing and splitting the text - see the Book by Chollet\n",
        "    # standardize=custom_standardization_fn,\n",
        "    # split=custom_split_fn,\n",
        ")\n",
        "\n",
        "# Adapt to data\n",
        "text_vectorization.adapt(dataset)      # Computes a vocabulary of string terms from tokens in a dataset\n"
      ],
      "metadata": {
        "id": "q0AS4G6RqMEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the working of TextVectorization\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "print(f\"vocabulary = {vocabulary}\")\n",
        "print(f\"len(vocabulary) = {len(vocabulary)}\")"
      ],
      "metadata": {
        "id": "IgWtoqFba5mc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7b0bab-2018-4371-e4b7-ab3a0c36b0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To see how the the text_vec layer transforms/vectorizes the raw text\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(f\"encoded sentence = {encoded_sentence}\")\n",
        "print(f\"len(encoded sentence) = {len(encoded_sentence)}\")\n",
        "# print(f\"encoded dataset_t = {text_vectorization(dataset_t)}\")"
      ],
      "metadata": {
        "id": "z-C1V5v7a-ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9121f8-7944-4dcf-bcfa-8fbcec25d840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded sentence = [ 7  3  5  9  1  5 10  0  0  0]\n",
            "len(encoded sentence) = 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decode back for comparison with test_sentence\n",
        "inverse_vocab = dict(enumerate(vocabulary)) # making a dictionary to decode embeddings\n",
        "print(f\"inverse_vocab = {inverse_vocab}\")\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(f\"decoded sentence = {decoded_sentence}\")"
      ],
      "metadata": {
        "id": "_7qHlt2ZbCYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6736bd62-2143-44ac-d32e-b366578762bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inverse_vocab = {0: '', 1: '[UNK]', 2: 'erase', 3: 'write', 4: 'then', 5: 'rewrite', 6: 'poppy', 7: 'i', 8: 'blooms', 9: 'and', 10: 'again', 11: 'a'}\n",
            "decoded sentence = i write rewrite and [UNK] rewrite again   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"test_sentence = {test_sentence}\")"
      ],
      "metadata": {
        "id": "eqL6sCaWTnTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f023b1d-91ca-4fb4-e1a7-02a1a51455bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing the dataset using TextVectorization layer of keras"
      ],
      "metadata": {
        "id": "flOZCU1JQNYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List subdirectories\n",
        "!cd aclImdb && ls -d */"
      ],
      "metadata": {
        "id": "46rYfHyZrqbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d2b5fa-164b-4e78-b423-da86591e611a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test/  train/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary folder\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "mFgcefQNLh-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise a sample\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "id": "-DWQXmaYLuha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0c562c-c0b2-452d-975e-310e9e128073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a validation directory and move 20% of the train data to it"
      ],
      "metadata": {
        "id": "Za7BufC_2q_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# move 20% of the training data to the validation folder\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    # random.Random(1337).shuffle(files) # We should shuffle. Only commenting for demonstration\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ],
      "metadata": {
        "id": "9BseO3_sLukB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create batches of data using `text_dataset_from_directory`"
      ],
      "metadata": {
        "id": "YBeeGi5S8pbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset using utility\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
        "\n",
        "val_ds = text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
        "\n",
        "test_ds = text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)\n",
        "\n",
        "# Extracting only the review text(not labels); to be used later to adapt the TextVec layer\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)             # lambda x, y: x  --> replace x,y with x. That is remove labels, just keep text data.\n"
      ],
      "metadata": {
        "id": "cDLoEh2CLwB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0da1a0d-209a-407e-d18e-476bf72e0ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 20000, 5000, and 25000 records in train, validation, and test directories with two class as positive and negative."
      ],
      "metadata": {
        "id": "htOQeKY98_3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shapes\n",
        "\n",
        "for inputs, targets in train_ds:\n",
        "\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "\n",
        "    print(\"inputs[2]:\", inputs[2])\n",
        "    print(\"targets[2]:\", targets[2])\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "u_03-Oj9LwD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06483011-def1-4d39-ab3f-e4fa34b2ad2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[2]: tf.Tensor(b'*SPOILERS INCLUDED*<br /><br />With a title like \"Bleed\", you know the creative juices weren\\'t running on high when this puppy was conceived. The movie is your basic run-of-the-mill low-budget slasher movie. Oh sure, it tries to be creative with the premise of the \"murder club\", but we learn that was just a joke anyways. Okay, for those who really care about these things, the basic plot is that new girl in town starts dating her co-worker. He invites her into his circle of friends, and at a party, they tell her how they have a \"murder club\" and they murder people, blah blah blah. Well, we learn that it was all a joke, but not before our heroine kills a lady in a parking garage. Now, the \"members\" of the Murder Club are being killed one by one. Oh, and the bad guys wins and the movie ends on a downer. By that time, you won\\'t really care though.<br /><br />In retrospect, the first 10 or so minutes of this movie make no sense. The motivation for the killings in the beginning of the movie is never explained. I would say that it was a way for the director to pad out the film, but on the DVD there are deleted scenes! I\\'m not sure why anyone would want to see more than the feature length version of \"Bleed\", but apparently the people behind the DVD thought the viewers would be clamoring for more. On the box, it says there are Easter Eggs, but why the hell I would want to waste my time looking for extras on this movie is beyond me. <br /><br />I was expecting a bad movie, and \"Bleed\" delivered on that front. It wasn\\'t a fun bad movie though. Everyone looks good in the movie, and there\\'s plenty of nudity, but the acting is just awful. My least favorite character is the guy who ends up being the killer...I think he\\'s supposed to be funny and amusing, but he just ends up coming off as a tool. I think the funniest moment of the movie is when our heroine kills the lady in the parking garage, in a hilariously unconvincing death. Heroine shoves the women into the parking garage cement pole, and the woman looks like she barely hits the thing, and she spits out a mouthful of blood, and \"dies\". <br /><br />For those who think that movie making is an intricate, creative process done by professionals, check out \"Bleed\". It will change your mind, and you\\'ll realize any hack get can a movie made. <br /><br />Otherwise, don\\'t waste your time or money on this.', shape=(), dtype=string)\n",
            "targets[2]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create TextVectorization layer and adapt to dataset"
      ],
      "metadata": {
        "id": "sR63E5Ea9fDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the data\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        "    )\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "\n",
        "# Apply TextVec to train, val, test set\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), tf.reshape(y, (-1,1))),\n",
        "                            num_parallel_calls=4)\n",
        "\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), tf.reshape(y, (-1,1))),\n",
        "                        num_parallel_calls=4)\n",
        "\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), tf.reshape(y, (-1,1))),\n",
        "                          num_parallel_calls=4)\n"
      ],
      "metadata": {
        "id": "Bqi5Z24gMK9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize and compare the raw and processed data"
      ],
      "metadata": {
        "id": "J7sj-tcc9v2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize the raw text and the vectorized (to int) text\n",
        "for text, label in train_ds:\n",
        "  print(text[0])\n",
        "  print(label[0])\n",
        "  break\n",
        "\n",
        "for int_of_text, label in int_train_ds:\n",
        "  print(int_of_text[0])\n",
        "  print(label[0])\n",
        "  break\n"
      ],
      "metadata": {
        "id": "nMUYGUqBx3K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector representation of the word 'movie'"
      ],
      "metadata": {
        "id": "P01-UwZIAuzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization(\"movie\")"
      ],
      "metadata": {
        "id": "Jl8iSY55z1A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector representation of \"great movie\" and \"a fine story\""
      ],
      "metadata": {
        "id": "6oAL6gzUAzvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization([\"great movie\", \"a fine story\"])"
      ],
      "metadata": {
        "id": "oCDXUf8bDRIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NN architecture with a TextVectorization layer, an Embedding layer, and Dense layers"
      ],
      "metadata": {
        "id": "k4mowLS1YiVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 20000\n",
        "inputs = keras.Input(shape=(1,), dtype=tf.string)           # shape=(None,), dtype=\"int64\"\n",
        "\n",
        "# The Text Vectoritation layer\n",
        "txt_vec_out = text_vectorization(inputs)             # Note that this TextVec layer is already apadted on the train dataset\n",
        "\n",
        "# The Embedding layer\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, name='embedding')(txt_vec_out)    # the largest integer (i.e. word index) in the input\n",
        "\n",
        "x = layers.Dense(64, activation=\"relu\")(embedded)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7tm5OHz0Ha1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embedding layer\n",
        "embedding_layer = model.get_layer('embedding')\n",
        "\n",
        "# Get the embeddings\n",
        "embeddings = embedding_layer.get_weights()[0]\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "Sr0WUiNtIFUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vocabulary from the TextVectorization layer\n",
        "vocab = text_vectorization.get_vocabulary()\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "WcbbZGCNRXgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample words to visualize word embeddings for\n",
        "test_words = ['good', 'bad', 'nice', 'poor', 'terrible', 'terrific', 'awesome', 'awful', 'best', 'worst']\n",
        "\n",
        "print(f\"{'Word':<15} {'Index'}\")\n",
        "print(\"=\"*30)\n",
        "for word in test_words:\n",
        "    print(f\"{word:<15} {vocab.index(word)}\")"
      ],
      "metadata": {
        "id": "D3RCOsGSRXgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding dimension\n",
        "embeddings[vocab.index('good')].shape"
      ],
      "metadata": {
        "id": "G0pgCNGOIFUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
        "# n_components in PCA specifies the no. of dimensions\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "\n",
        "# Fit and transform the vectors using PCA model\n",
        "reduced_untrained_emb = pca.fit_transform(embeddings)\n",
        "reduced_untrained_emb.shape"
      ],
      "metadata": {
        "id": "pjxK8A-o6Cdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduced embedding for word 'good'\n",
        "reduced_untrained_emb[vocab.index('good')]"
      ],
      "metadata": {
        "id": "-SJ4bmMPIFUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the embeddings\n",
        "plt.figure(figsize=(8, 6))\n",
        "for word in test_words:\n",
        "    if word != '':  # Skip the empty string token\n",
        "        x, y = reduced_untrained_emb[vocab.index(word)]\n",
        "        plt.scatter(x, y)\n",
        "        plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points')\n",
        "\n",
        "plt.title(\"Word Embeddings Visualization (Before training)\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mOYsvNeNbjcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on train set\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\"one_hot_dense.keras\", save_best_only=True)]\n",
        "\n",
        "# Change target shape from (None,) to (None, 1)\n",
        "train_dataset = train_ds.map(lambda x, y: (x, tf.reshape(y, (-1,1))))\n",
        "val_dataset = val_ds.map(lambda x, y: (x, tf.reshape(y, (-1,1))))\n",
        "\n",
        "model.fit(train_dataset,\n",
        "          validation_data = val_dataset,\n",
        "          epochs = 20,\n",
        "          callbacks = callbacks)"
      ],
      "metadata": {
        "id": "WGtVcVbnIFUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load saved model\n",
        "# model = keras.models.load_model(\"one_hot_dense.keras\")\n",
        "\n",
        "# Check model performance\n",
        "test_dataset = test_ds.map(lambda x, y: (x, tf.reshape(y, (-1,1))))\n",
        "print(f\"Test acc: {model.evaluate(test_dataset)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "qyC4SWhR3k1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embedding layer\n",
        "trained_embedding_layer = model.get_layer('embedding')\n",
        "\n",
        "# Get the embeddings\n",
        "trained_embeddings = trained_embedding_layer.get_weights()[0]\n",
        "trained_embeddings.shape"
      ],
      "metadata": {
        "id": "D8G_TRTfIFUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
        "# n_components in PCA specifies the no.of dimensions\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "\n",
        "# Fit and transform the vectors using PCA model\n",
        "reduced_trained_emb = pca.fit_transform(trained_embeddings)\n",
        "reduced_trained_emb.shape"
      ],
      "metadata": {
        "id": "hrsYQ-5Y58nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the embeddings\n",
        "plt.figure(figsize=(10, 8))\n",
        "for word in test_words:\n",
        "    if word != '':  # Skip the empty string token\n",
        "        x, y = reduced_trained_emb[vocab.index(word)]\n",
        "        plt.scatter(x, y)\n",
        "        plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points')\n",
        "\n",
        "plt.title(\"Word Embeddings Visualization (After training)\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-iiGLD_RIFUk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}